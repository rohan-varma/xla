module @IrToHlo.80 attributes {mhlo.cross_program_prefetches = [], mhlo.dynamic_parameter_bindings = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<f32> loc(unknown), %arg1: tensor<2x3xf32> loc(unknown), %arg2: tensor<2x3xf32> loc(unknown), %arg3: tensor<2x3xf32> loc(unknown)) -> tensor<2x3xf32> {
    %0 = stablehlo.constant dense<0.000000e+00> : tensor<f32> loc(#loc23)
    %1 = stablehlo.constant dense<0.000000e+00> : tensor<2x3xf32> loc(#loc24)
    %2 = stablehlo.exponential %arg3 : tensor<2x3xf32> loc(#loc25)
    %3 = stablehlo.exponential %arg2 : tensor<2x3xf32> loc(#loc26)
    %4 = stablehlo.constant dense<1.000000e+00> : tensor<f32> loc(#loc27)
    %5 = stablehlo.constant dense<1.000000e+00> : tensor<1x1xf32> loc(#loc28)
    %6 = stablehlo.reshape %5 : (tensor<1x1xf32>) -> tensor<f32> loc(#loc28)
    %7 = stablehlo.broadcast_in_dim %6, dims = [] : (tensor<f32>) -> tensor<2x3xf32> loc(#loc28)
    %8 = stablehlo.multiply %3, %7 : tensor<2x3xf32> loc(#loc29)
    %9 = stablehlo.add %2, %8 : tensor<2x3xf32> loc(#loc30)
    %10 = stablehlo.abs %9 : tensor<2x3xf32> loc(#loc31)
    %11 = stablehlo.constant dense<0xFF800000> : tensor<f32> loc(#loc32)
    %12 = stablehlo.reduce(%10 init: %11) across dimensions = [1] : (tensor<2x3xf32>, tensor<f32>) -> tensor<2xf32>
     reducer(%arg4: tensor<f32> loc(unknown), %arg5: tensor<f32> loc(unknown))  {
      %41 = stablehlo.maximum %arg4, %arg5 : tensor<f32> loc(#loc15)
      stablehlo.return %41 : tensor<f32> loc(#loc)
    } loc(#loc32)
    %13 = stablehlo.broadcast_in_dim %12, dims = [0] : (tensor<2xf32>) -> tensor<2x3xf32> loc(#loc32)
    %14 = stablehlo.subtract %10, %13 : tensor<2x3xf32> loc(#loc32)
    %15 = stablehlo.exponential %14 : tensor<2x3xf32> loc(#loc32)
    %16 = stablehlo.reduce(%15 init: %0) across dimensions = [1] : (tensor<2x3xf32>, tensor<f32>) -> tensor<2xf32>
     reducer(%arg4: tensor<f32> loc(unknown), %arg5: tensor<f32> loc(unknown))  {
      %41 = stablehlo.add %arg4, %arg5 : tensor<f32> loc(#loc16)
      stablehlo.return %41 : tensor<f32> loc(#loc)
    } loc(#loc32)
    %17 = stablehlo.log %16 : tensor<2xf32> loc(#loc32)
    %18 = stablehlo.broadcast_in_dim %17, dims = [0] : (tensor<2xf32>) -> tensor<2x3xf32> loc(#loc32)
    %19 = stablehlo.subtract %14, %18 : tensor<2x3xf32> loc(#loc32)
    %20 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<2x3xf32> loc(#loc24)
    %21 = stablehlo.clamp %1, %19, %20 : tensor<2x3xf32> loc(#loc24)
    %22 = stablehlo.log %21 : tensor<2x3xf32> loc(#loc33)
    %23 = stablehlo.abs %22 : tensor<2x3xf32> loc(#loc34)
    %24 = stablehlo.exponential %arg1 : tensor<2x3xf32> loc(#loc35)
    %25 = stablehlo.multiply %24, %7 : tensor<2x3xf32> loc(#loc29)
    %26 = stablehlo.add %3, %25 : tensor<2x3xf32> loc(#loc30)
    %27 = stablehlo.abs %26 : tensor<2x3xf32> loc(#loc31)
    %28 = stablehlo.reduce(%27 init: %11) across dimensions = [1] : (tensor<2x3xf32>, tensor<f32>) -> tensor<2xf32>
     reducer(%arg4: tensor<f32> loc(unknown), %arg5: tensor<f32> loc(unknown))  {
      %41 = stablehlo.maximum %arg4, %arg5 : tensor<f32> loc(#loc20)
      stablehlo.return %41 : tensor<f32> loc(#loc)
    } loc(#loc32)
    %29 = stablehlo.broadcast_in_dim %28, dims = [0] : (tensor<2xf32>) -> tensor<2x3xf32> loc(#loc32)
    %30 = stablehlo.subtract %27, %29 : tensor<2x3xf32> loc(#loc32)
    %31 = stablehlo.exponential %30 : tensor<2x3xf32> loc(#loc32)
    %32 = stablehlo.reduce(%31 init: %0) across dimensions = [1] : (tensor<2x3xf32>, tensor<f32>) -> tensor<2xf32>
     reducer(%arg4: tensor<f32> loc(unknown), %arg5: tensor<f32> loc(unknown))  {
      %41 = stablehlo.add %arg4, %arg5 : tensor<f32> loc(#loc21)
      stablehlo.return %41 : tensor<f32> loc(#loc)
    } loc(#loc32)
    %33 = stablehlo.log %32 : tensor<2xf32> loc(#loc32)
    %34 = stablehlo.broadcast_in_dim %33, dims = [0] : (tensor<2xf32>) -> tensor<2x3xf32> loc(#loc32)
    %35 = stablehlo.subtract %30, %34 : tensor<2x3xf32> loc(#loc32)
    %36 = stablehlo.clamp %1, %35, %20 : tensor<2x3xf32> loc(#loc24)
    %37 = stablehlo.log %36 : tensor<2x3xf32> loc(#loc36)
    %38 = stablehlo.abs %37 : tensor<2x3xf32> loc(#loc34)
    %39 = stablehlo.multiply %38, %7 : tensor<2x3xf32> loc(#loc37)
    %40 = stablehlo.add %23, %39 : tensor<2x3xf32> loc(#loc38)
    return %40 : tensor<2x3xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("prim__Constant")
#loc2 = loc("hardtanh@functional.py":1522:0)
#loc3 = loc("aten__clamp")
#loc4 = loc("[{\22name\22: \22my_tag\22, \22pos\22: 0, \22id\22: 0, \22is_input\22: true}]")
#loc5 = loc("__call__@_ops.py":435:0)
#loc6 = loc("[{\22name\22: \22my_tag\22, \22pos\22: 1, \22id\22: 0, \22is_input\22: true},{\22name\22: \22my_tag\22, \22pos\22: 0, \22id\22: 1, \22is_input\22: true}]")
#loc7 = loc("my_softmax@test_tag.py":13:0)
#loc8 = loc("aten__expand")
#loc9 = loc("aten__mul")
#loc10 = loc("aten__add")
#loc11 = loc("aten__abs")
#loc12 = loc("my_softmax@test_tag.py":14:0)
#loc13 = loc("aten__log_softmax")
#loc14 = loc("log_softmax@functional.py":1945:0)
#loc15 = loc("maximum.59")
#loc16 = loc("add.68")
#loc17 = loc("[{\22name\22: \22my_tag\22, \22pos\22: 0, \22id\22: 0, \22is_input\22: false}]")
#loc18 = loc("my_softmax@test_tag.py":17:0)
#loc19 = loc("[{\22name\22: \22my_tag\22, \22pos\22: 1, \22id\22: 1, \22is_input\22: true}]")
#loc20 = loc("maximum.24")
#loc21 = loc("add.33")
#loc22 = loc("[{\22name\22: \22my_tag\22, \22pos\22: 0, \22id\22: 1, \22is_input\22: false}]")
#loc23 = loc(fused[#loc1, #loc2])
#loc24 = loc(fused[#loc3, #loc2])
#loc25 = loc(fused[#loc4, #loc5])
#loc26 = loc(fused[#loc6, #loc5])
#loc27 = loc(fused[#loc1, #loc7])
#loc28 = loc(fused[#loc8, #loc7])
#loc29 = loc(fused[#loc9, #loc7])
#loc30 = loc(fused[#loc10, #loc7])
#loc31 = loc(fused[#loc11, #loc12])
#loc32 = loc(fused[#loc13, #loc14])
#loc33 = loc(fused[#loc17, #loc18])
#loc34 = loc(fused[#loc11, #loc5])
#loc35 = loc(fused[#loc19, #loc5])
#loc36 = loc(fused[#loc22, #loc18])
#loc37 = loc(fused[#loc9, #loc5])
#loc38 = loc(fused[#loc10, #loc5])